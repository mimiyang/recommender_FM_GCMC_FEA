\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{ntheorem}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{url}
\usepackage{todonotes}
\usepackage{amsbsy}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{amsmath}%
\usepackage{MnSymbol}%
\usepackage{wasysym}%
\usepackage[cal=dutchcal,
calscaled=1,
bb=boondox,
scr=euler]{mathalfa}

\usepackage[capitalise]{cleveref}
\crefformat{equation}{(#2#1#3)}

\newcommand{\ie}[0]{\emph{i.e.},~}
\newcommand{\eg}[0]{\emph{e.g.},~}
\newcommand{\aka}[0]{a.k.a.~}
\newcommand{\etal}{\emph{et al.~}}
\newcommand{\etc}{\emph{etc.~}}
\newcommand{\wrt}{w.r.t.~}
\def\todo#1{{\color{red}{{todo:\ #1\ }}}}
\newcommand{\defeq}{\ensuremath{\doteq}}

\newcommand{\mat}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\gr}[1]{\ensuremath{\mathcal{#1}}}
\newcommand{\set}[1]{\ensuremath{\mathbb{#1}}}
\renewcommand{\vec}[1]{\ensuremath{\overrightarrow{#1}}}
\newcommand{\prm}[1]{\ensuremath{^{(#1)}}}
\newcommand{\ttt}[1]{\ensuremath{^{(#1)}}}
\newcommand{\grn}[2]{\ensuremath{\gr{#1}\prm{#2}}}
\newcommand{\Trp}[0]{\ensuremath{^{\mathsf{T}}}}
\newcommand{\A}[0]{\ensuremath{\mat{A}}}
\newcommand{\XX}[0]{\ensuremath{\mat{X}}}
\newcommand{\YY}[0]{\ensuremath{\mat{Y}}}
\newcommand{\ZZ}[0]{\ensuremath{\mat{Z}}}
\newcommand{\xx}[0]{\ensuremath{\mat{x}}}
\newcommand{\yy}[0]{\ensuremath{\mat{y}}}
\newcommand{\zz}[0]{\ensuremath{\mat{z}}}
\newcommand{\Xset}[0]{\ensuremath{\set{X}}}
\newcommand{\Yset}[0]{\ensuremath{\set{Y}}}
\newcommand{\Rset}[0]{\ensuremath{\set{R}}}
\renewcommand{\Re}[0]{\ensuremath{\set{R}}}
\newcommand{\thetas}[0]{\ensuremath{\boldsymbol{theta}}}
\newcommand{\Thetas}[0]{\ensuremath{\boldsymbol{Theta}}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{objective}[theorem]{Objective}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\theoremsymbol{\ensuremath{\blacksquare}}
\newtheorem*{proof}{Proof}%[section]

\title{Exchangeable Tensor Model}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
....
\end{abstract}
\section{Introduction}
We derive this for matrices and then generalize to tensors.
Also for now consider a single input/output channel.
Lets add relevant papers to the bibliography as we proceed.

\section{Notation}
\begin{itemize}
\item $\XX \in \Re^{N \times M}$
\item $\xx \in \Re^{N M}$: vectorized $\XX$, also denoted by $\vec{\XX}$.
\item $\A_{n,:} \in \Re^{1 \times M}$ is the $n^\text{th}$ row of any $\A \in \Re^{N \times M}$
\item $\A_{:,m} \in \Re^{N \times 1}$ is the $m^\text{th}$ column of any $\A \in \Re^{N \times M}$
\item $[N]$ denotes the sequence $\{n\}_{n=1,...,N} = (1, 2, ... N)$
\item $[N \times M]$ denotes the sequence $\{(n, m)\}_{n \in [N], m \in [M]} = \big((1,1), (2,1),..., (N, M)\big)$
\item $\grn{S}{N}$ symmetric group of \textit{all} permutations of $N$ objects
\item $\grn{G}{N} = \{\grn{g}{N}_i\}_i$ a permutation group of $N$ objects 
\item $\grn{g}{N}_i$ or $\mat{G}_i\prm{N}$ both can refer to the matrix form of the permutation $\grn{g}{N}_i \in \grn{G}{N}$.
\item $\grn{g}{N}_i(n)$ refers to the result of applying $\grn{g}{N}_i$ to $n$, for any $n \in [N]$.
\end{itemize}

\section{Problem}
$\grn{S}{N} \times \grn{S}{M} < \grn{S}{N M}$ is the product of group of all permutations of $N$ and $M$ objects. It has $N! M!$ members compared to $(NM)!$ members of the group $\gr{S}_{N M}$.
The members $\grn{g}{N,M} \in \grn{S}{N}$ can be represented by matrices $\mat{G}\prm{N M}$. This matrix ``acts'' on the vector $\vec{\XX}$.
Alternatively we can represent them using the pair $(\mat{G}\prm{N}, \mat{G}\prm{M})$ where the ``action'' of this pair is defined on the matrix $\XX$:
\begin{align*}
  \mat{G}\prm{N N \times M M} \vec{\XX} \equiv \vec{\mat{G}\prm{N} \XX \mat{G}\prm{M}}
\end{align*}

\begin{objective}\label{obj:1}
  Define a parameter matrix $\Theta \in \Re^{ N N \times M M}$ such that
  \begin{itemize}
  \item \textbf{Equivariance} (adding nonlinearity does not change anything):
  \begin{align}
    \mat{G}\prm{N M} \Theta \vec{\XX} = \Theta \mat{G}\prm{N M} \vec{\XX}\quad \forall \vec{\XX} \in \Re^{N M}, \mat{G}\prm{N M} \in \grn{S}{N} \times \grn{S}{M}
  \end{align}
\item \textbf{No equivariance wrt any other permutation}:
  For any permutation of elements of the matrix $\XX$ that is not produced using a permutation of rows and columns $\mat{H} \in \grn{S}{N M}, \mat{H} \notin \grn{S}{N} \times \grn{S}{M}$, the
  parameter matrix $\Theta$ should have the property that
  \begin{align}
   \exists \XX \quad s.t. \quad  \mat{H}\prm{N M} \Theta \vec{\XX} \neq \Theta \mat{H}\prm{N M} \vec{\XX} 
  \end{align}
  These two necessary and sufficient conditions of equivariance strictly define the form of $\Theta$.
\end{itemize}
\end{objective}


\section{A Proposal}
Here is a proposal for $\Theta \in \Re^{NM \times NM}$:
\begin{align}
  \Theta_{(n,m), (n',m')} \defeq
  \begin{cases}
    \theta_1 & n = n', m = m'\\
    \theta_2 & n = n', m \neq m' \\
    \theta_3 & n \neq n', m = m' \\
    \theta_4 & n \neq n', m \neq m' \\
  \end{cases}
\end{align}
Intuitively, each element ${m,n}$ at the output interacts with 1) corresponding input element; 2) all elements of the input in its row; 3) its column; 4) all other elements of $\XX$.

We can write the product $\Theta \vec{X}$ as
\begin{align}\label{eq:param}
  \Theta * \XX \defeq \Theta \vec{X} = \theta'_1 \XX +  \theta'_2 \mat{1} (\mat{1}\Trp \XX) + \theta'_3 (\XX \mat{1})\mat{1}\Trp + \theta'_4 (\mat{1}\Trp \XX \mat{1})
\end{align}
where $\mat{1} = [1,\ldots,1]\Trp$ is the a column vector of appropriate size.

Replacing the summation with average we get:
\begin{align}\label{eq:param_avg}
  \Theta * \XX \defeq \Theta \vec{X} = \theta''_1 \XX + \frac{\theta''_2}{N} \mat{1} (\mat{1}\Trp \XX) + \frac{\theta''_3}{M} (\XX \mat{1})\mat{1}\Trp + \frac{\theta''_4}{N M} (\mat{1}\Trp \XX \mat{1})
\end{align}

We need to show that this matrix commutes with any permutation of rows and columns (Equivariance condition in the objective). This also follows as a Corollary of Proposition 3.1 in \citep{ravanbakhsh_symmetry}. However, it is not obvious whether the other direction (non-equivariance) holds or not. If it does not, it may still work well in practice.

\begin{proposition}\label{prop:H1}
  Let $\grn{h}{NM} \in \grn{S}{NM}$. If $\grn{h}{NM} \not\in \grn{S}{N} \times \grn{S}{M}$, then at least one of the following holds:
  \begin{enumerate}
    \item[(i)]
      There exist $n, n' \in [N]$ and $m, m' \in [M]$ such that 
      \begin{align*}
        \grn{h}{NM}((n, m)) &= (n', m), \quad \text{but} \\
        \grn{h}{NM}((n, m')) &\neq (n', m'). 
      \end{align*}
    \item[(ii)]
      There exist $n, n' \in [N]$ and $m, m' \in [M]$ such that 
      \begin{align*}
        \grn{h}{NM}((n, m)) &= (n, m'), \quad \text{but} \\
        \grn{h}{NM}((n', m)) &\neq (n', m'). 
      \end{align*}
  \end{enumerate}
\end{proposition}
If we think of $\grn{h}{NM}$ as operating on $\vec{\XX}$, the vectorization of some matrix $\XX \in \Re^{N \times M}$, then item (i) says that $\grn{h}{NM}$ `breaks' the $n^\text{th}$ row of $\XX$ (i.e., $\grn{h}{NM}$ maps the $m^\text{th}$ and $m'$$^\text{th}$ elements of row $n$ to different rows). Similarly, item (ii) says that $\grn{h}{NM}$ `breaks' the $m^\text{th}$ column of $\XX$ by mapping elements $n$ and $n'$ to different columns. Intuitively, this means that $\mat{H}\prm{NM} \vec{\XX}$ cannot be expressed as $\vec{\mat{H}\prm{N} \XX \mat{H}\prm{M}}$ for any $\mat{H}\prm{N} \in \grn{S}{N}$, $\mat{H}\prm{M} \in \grn{S}{M}$.

\begin{proof}
We prove the contrapositive. Suppose neither (i) nor (ii) holds. Let $m$ and $m'$ be the indices of two particular columns of $\XX$. The negation of item (i) says that $\grn{h}{NM}$ applies the same mapping to each element of $m$ as it does to each element of $m'$. Call this mapping $\pi_r$ and note that $\pi_r \in \grn{S}{N}$. Similarly, let $n$ and $n'$ be two particular rows of $\XX$. The negation of (ii) says that $\grn{h}{NM}$ applies the same mapping to each element of $n$ as it does to each element of $n'$. Call this mapping $\pi_c$, and note that $\pi_c \in \grn{S}{M}$. Thus, we have that $\grn{h}{NM}( \vec{\XX}) = \pi_r \XX \pi_c$ and so $\grn{h}{NM} \in \grn{S}{N} \times \grn{S}{M}$, completing the proof.
\end{proof}
\todo{maybe describe connection to Kronecker product?} \\

Note that, if $\grn{h}{NM} \in \grn{S}{NM}$ is any permutation, and $\mat{H}\prm{NM} $ is the associated matrix, then it is easily verified that:
\begin{align}\label{eqn:H}
  \grn{h}{NM}((n, m)) = (n', m') &\iff \big( \mat{H}\prm{NM}  \big)_{(n',m'), (n,m)} = 1
\end{align}
for any $n,n' \in [N]$ and $m,m' \in [M]$.

\begin{proposition}\label{prop:H2}
Let $\grn{h}{NM} \in \grn{S}{NM}$, and $\grn{h}{NM} \not\in \grn{S}{N} \times \grn{S}{M}$, with $\mat{H}\prm{NM} \in \{0,1\}^{NM \times NM}$ the corresponding matrix. Let $\Theta \in \Re^{NM \times NM}$ be as described above. Then the following hold:
  \begin{enumerate}
    \item[(i)]
      If there exist $n, n' \in [N]$ and $m, m' \in [M]$ such that 
      \begin{align*}
        \grn{h}{NM}((n, m)) &= (n', m), \quad \text{and} \\
        \grn{h}{NM}((n, m')) &\neq (n', m'), 
      \end{align*}
      then
      \begin{align*}
        \big( \mat{H}\prm{N M} \Theta \big)_{(n',m'), (n,m)} \neq \big( \Theta \mat{H}\prm{N M} \big)_{(n',m'), (n,m)}
      \end{align*}
    \item[(ii)]
      If there exist $n, n' \in [N]$ and $m, m' \in [M]$ such that 
      \begin{align*}
        \grn{h}{NM}((n, m)) &= (n, m'), \quad \text{and} \\
        \grn{h}{NM}((n', m)) &\neq (n', m'), 
      \end{align*}
      then
      \begin{align*}
        \big( \mat{H}\prm{N M} \Theta \big)_{(n,m'), (n',m)} \neq \big( \Theta \mat{H}\prm{N M} \big)_{(n,m'), (n',m)}
      \end{align*}
  \end{enumerate}
\end{proposition}
\begin{proof}
  We prove items (i) and (ii) in turn:
  \begin{enumerate}
  
  
    \item[(i)] First, note that if $\grn{h}{NM}((n, m')) \neq (n', m')$, then by the observation (\ref{eqn:H}) above $H_{(n',m'), (n,m')} \neq 1$. Thus, we have
      \begin{align*}
        \big( \mat{H}\prm{N M} \Theta \big)_{(n',m'), (n,m)} &= \big(\mat{H}\prm{N M}\big)_{(n',m'),:} \big(\Theta \big)_{:,(n,m)} \\
        &= \sum_{(i,j) \in [N \times M]} \big(\mat{H}\prm{N M}\big)_{(n',m'),(i,j)} \big(\Theta \big)_{(i,j),(n,m)} \\
        &\neq \Theta_{(n,m'),(n,m)} \\
        &= \theta_2
      \end{align*}
      
      But also by observation (\ref{eqn:H}) above, since $\grn{h}{NM}((n, m)) = (n', m)$ , we have $H_{(n',m), (n,m)} = 1$. And so
      \begin{align*}
        \big(\Theta \mat{H}\prm{N M} \big)_{(n',m'), (n,m)} &= \big(\Theta \big)_{(n',m'), :} \big(\mat{H}\prm{N M} \big)_{:, (n,m)} \\
        &= \sum_{(i,j) \in [N \times M]} \big(\Theta \big)_{(n',m'), (i,j)} \big(\mat{H}\prm{N M} \big)_{(i,j), (n,m)} \\
        &= \Theta_{(n', m'), (n', m)} \\
        &= \theta_2
       \end{align*}
    This proves item (i). 
    
    
    
    
    \item[(ii)]
      Similarly, if $\grn{h}{NM}((n, m)) = (n, m')$ then $H_{(n,m'), (n,m)} = 1$ and so
      \begin{align*}
        \big( \mat{H}\prm{N M} \Theta \big)_{(n,m'), (n',m)} &= \big(\mat{H}\prm{N M}\big)_{(n,m'),:} \big(\Theta \big)_{:,(n',m)} \\
        &= \sum_{(i,j) \in [N \times M]} \big(\mat{H}\prm{N M}\big)_{(n,m'),(i,j)} \big(\Theta \big)_{(i,j),(n',m)} \\
        &= \Theta_{(n,m), (n',m)} \\
        &= \theta_3
      \end{align*}
      
        And since $\grn{h}{NM}((n', m)) \neq (n', m')$, we have $H_{(n',m'), (n',m)} \neq 1$, and 
      \begin{align*}
         \big(\Theta \mat{H}\prm{N M} \big)_{(n,m'), (n',m)} &= \big(\Theta \big)_{(n,m'), :} \big(\mat{H}\prm{N M} \big)_{:, (n',m)} \\
         &= \sum_{(i,j) \in [N \times M]} \big(\Theta \big)_{(n,m'), (i,j)} \big(\mat{H}\prm{N M} \big)_{(i,j), (n',m)} \\
         &\neq \Theta_{(n,m'), (n',m')} \\
         &= \theta_3
      \end{align*}
      Which proves item (ii) and thus the Proposition. 
   \end{enumerate}
   
   
\end{proof}

\begin{theorem}
  Parameter matrix $\Theta$ of \ref{eq:param} satisfies the conditions of Objective~\ref{obj:1}.
\end{theorem}
\begin{proof}
  Let $\XX \in \Re^{N \times M}$. We will prove that both points of Objective~\ref{obj:1} are satisfied by $\Theta$. 
  \begin{itemize}
    \item \textbf{Equivariance}: Let $\grn{g}{NM} \in \grn{S}{N} \times \grn{S}{M}$, and let $\mat{G}\prm{NM}$ be the corresponding matrix representation. Then ... \todo{finish this part}
    \item \textbf{No equivariance wrt any other permutation}: Let $\grn{h}{NM} \in \grn{S}{NM}$, with $\grn{h}{NM} \not\in \grn{S}{N} \times \grn{S}{M}$, and let $\mat{H}\prm{NM}$ be the corresponding matrix representation. Then an immediate consequence of Propositions \ref{prop:H1} and \ref{prop:H2} is that 
  \begin{align*}
     \mat{H}\prm{N M} \Theta \neq \Theta \mat{H}\prm{N M}
  \end{align*}
  \end{itemize}
  Thus both points of Objective~\ref{obj:1} are satisfied, proving the theorem. 
  
\end{proof}

In practice, we substitute the summation with either average or max operation. This does not change equivariance properties.
%\todo{multiple channels}
\todo{any relation to low-rank completion?}

\subsection{Multiple Channels}
% In this setting $\XX \in \Re^{N \times M \times K}$, and parameters $\thetas_1,\ldots,\thetas_4 \in \Re^{K \times L}$, where $L$ is the number of output
% channels. We use the lower-case of letters $N,M,K,L$ for indexing and write the layer for $\max$ operation instead of sum in \cref{eq:param}.
% \begin{align}
%   \label{eq:multi}
%   \XX_{m,n,l} \defeq \XX_{m,n,l} \thetas_{k,l} + 
% \end{align}

\section{Extension to Tensors}

% \section{Applications}
% \subsection{Tensor Completion}\todo{find applications}

% \subsection{Matrix Completion}
% Given partially observed tensor $\XX \in \Re^{N \times M \times K}$, where $K$ is the number of input-channels, we seek to estimate the unobserved values.
% Exchangeability is only within rows and columns (not channels).
% We use $\set{O} = \{(m,n)\}$ denote the set of observed indices. Here we have the option of 1) producing the output for \textit{all} entries ($m,n$)
% in \cref{eq:param} starting from the first layer, or 2) produce it only at the final layer or 3) only produce the output for entries $\set{Q} = \{m,n\}$
% where a query is made.
% Note that depending on this choice the layer \cref{eq:param} would change to only ``use'' the observed entries $\set{O}$ and ``produce'' the required ones $\set{Q}$.

% In collaborative filtering applications, the rows ($n$) represent customers and columns ($m$)
% represent products. In addition to customer/product ratings we may have feature-vectors $\yy \in \Re^{N \times K'}$ and
% $\zz \in \Re^{M \times K''}$ for each customer and product. Assuming $K' = K'' = 1$, the output of \cref{eq:param} is extended to incorporate
% these using $\theta_5 \yy \mat{1}\Trp + \theta_6 \mat{1} \zz\Trp$. For multiple input/output channels this needs to be extended.
% We also have the option of defining the layer such that we also have outputs corresponding to $\yy$ and $\zz$, in addition to the matrix output of $\XX$.

% Consider a deep model where the output of the final layer is $\YY \in \Re^{N \times M \times K}$,
% where $K=1$ shows that the last layer has a single channel.
% The loss function minimized through back-propagation is
% $\sum_{(m,n) \in \set{O}} \ell(X_{(m,n)}, \YY_{(m,n)})$, where $\ell$ is an appropriate matching loss, such as $L_2$ loss.

\section{ToDo List}
Theoretical side:
\begin{enumerate}
\item Try to prove the theorem. Does it satisfy the second part of Objective? 
\item If not see whether a better parameter-sharing scheme exists.
% \item Write it for multiple channels
% \item Extend to tensors using proper notation
\end{enumerate}

% Implementation:
% \begin{enumerate}
% \item Implement this layer for dense input/output matrices
% \item Implement it for Sparse input output, when the sets $\set{O}$ and $\set{Q}$ are given
% \item Apply it to benchmark datasets (\eg movie-lens?) for matrix completion
% \item What type of layer performs better for completion?
% \end{enumerate}


\bibliographystyle{abbrvnat} % sorted alphabetically, labeled with numbers
\nocite{*}
{\bibliography{refs.bib}} % names file keylatex.bib as my bibliography file
\end{document}




     


